name: Memory Pool Performance Benchmarks

on:
  push:
    branches: [ main, claude/* ]
    paths:
      - 'internal/memory_pool.h'
      - 'tests/memory_pool_benchmark.cpp'
      - 'tests/memory_pool_tests.cpp'
      - '.github/workflows/memory-pool-benchmarks.yml'
  pull_request:
    branches: [ main ]
    paths:
      - 'internal/memory_pool.h'
      - 'tests/memory_pool_benchmark.cpp'
  workflow_dispatch:
    inputs:
      update_docs:
        description: 'Update performance documentation'
        required: false
        default: 'true'
        type: choice
        options:
          - 'true'
          - 'false'

permissions:
  contents: write
  pull-requests: write

jobs:
  benchmark-memory-pool:
    name: Memory Pool Benchmarks (${{ matrix.os }})
    runs-on: ${{ matrix.os }}
    timeout-minutes: 20

    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-22.04, macos-latest]
        compiler: [clang]
        build_type: [Release]

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: Install dependencies (Ubuntu)
      if: runner.os == 'Linux'
      run: |
        sudo apt-get update
        sudo apt-get install -y cmake g++-12 ninja-build libbenchmark-dev libgtest-dev libfmt-dev
        sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-12 100
        sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-12 100

    - name: Install dependencies (macOS)
      if: runner.os == 'macOS'
      run: |
        brew install ninja google-benchmark googletest fmt

    - name: Set up compiler
      run: |
        echo "CC=clang" >> $GITHUB_ENV
        echo "CXX=clang++" >> $GITHUB_ENV

    - name: Configure CMake
      run: |
        cmake -B build -S . \
          -GNinja \
          -DCMAKE_BUILD_TYPE=${{ matrix.build_type }} \
          -DBUILD_TESTS=ON

    - name: Build memory pool benchmark
      run: |
        cmake --build build --target memory_pool_benchmark -j

    - name: Run memory pool benchmarks (JSON output)
      id: run_benchmark
      run: |
        cd build
        if [ -f ./memory_pool_benchmark ]; then
          ./memory_pool_benchmark \
            --benchmark_format=json \
            --benchmark_out=memory_pool_results_${{ matrix.os }}.json \
            --benchmark_repetitions=5 \
            --benchmark_report_aggregates_only=true
        elif [ -f ./bin/memory_pool_benchmark ]; then
          ./bin/memory_pool_benchmark \
            --benchmark_format=json \
            --benchmark_out=memory_pool_results_${{ matrix.os }}.json \
            --benchmark_repetitions=5 \
            --benchmark_report_aggregates_only=true
        else
          echo "::warning::memory_pool_benchmark not found - may need Google Benchmark library"
          exit 0
        fi

    - name: Run memory pool benchmarks (console output)
      run: |
        cd build
        BENCHMARK_BIN=""
        if [ -f ./memory_pool_benchmark ]; then
          BENCHMARK_BIN=./memory_pool_benchmark
        elif [ -f ./bin/memory_pool_benchmark ]; then
          BENCHMARK_BIN=./bin/memory_pool_benchmark
        fi

        if [ -n "$BENCHMARK_BIN" ]; then
          echo "=== Memory Pool Performance Benchmarks ==="
          $BENCHMARK_BIN \
            --benchmark_filter="BM_Pool.*64B|BM_Standard.*64B|BM_Pool.*1KB|BM_PoolConcurrent|BM_PoolWebServer" \
            --benchmark_repetitions=3 \
            --benchmark_report_aggregates_only=true
        fi

    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: memory-pool-results-${{ matrix.os }}
        path: build/memory_pool_results_${{ matrix.os }}.json
        retention-days: 90

    - name: Generate performance summary
      id: summary
      if: always()
      run: |
        cd build
        RESULTS_FILE=""
        if [ -f memory_pool_results_${{ matrix.os }}.json ]; then
          RESULTS_FILE=memory_pool_results_${{ matrix.os }}.json
        fi

        if [ -n "$RESULTS_FILE" ]; then
          echo "## Memory Pool Performance - ${{ matrix.os }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Benchmark | Time (ns) | Iterations |" >> $GITHUB_STEP_SUMMARY
          echo "|-----------|-----------|------------|" >> $GITHUB_STEP_SUMMARY

          # Parse JSON and extract key benchmarks
          python3 << 'EOF' >> $GITHUB_STEP_SUMMARY
import json
import sys

try:
    with open('$RESULTS_FILE', 'r') as f:
        data = json.load(f)

    for bench in data.get('benchmarks', []):
        name = bench.get('name', 'Unknown')
        time = bench.get('cpu_time', 0)
        iterations = bench.get('iterations', 0)

        # Filter key benchmarks
        if any(x in name for x in ['64B', '1KB', 'Concurrent', 'WebServer', 'Batch/64']):
            print(f"| {name} | {time:.1f} | {iterations} |")
except Exception as e:
    print(f"Error parsing results: {e}", file=sys.stderr)
EOF

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Platform**: ${{ matrix.os }}" >> $GITHUB_STEP_SUMMARY
          echo "**Build**: ${{ matrix.build_type }}" >> $GITHUB_STEP_SUMMARY
          echo "**Compiler**: ${{ matrix.compiler }}" >> $GITHUB_STEP_SUMMARY
        else
          echo "::warning::No benchmark results generated"
        fi

  update-documentation:
    name: Update Performance Documentation
    needs: benchmark-memory-pool
    runs-on: ubuntu-22.04
    if: |
      always() &&
      (github.event_name == 'push' && github.ref == 'refs/heads/main') ||
      (github.event_name == 'workflow_dispatch' && github.event.inputs.update_docs == 'true')

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: Download all benchmark results
      uses: actions/download-artifact@v4
      with:
        pattern: memory-pool-results-*
        path: benchmark-artifacts

    - name: Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Parse and update documentation
      id: update_docs
      run: |
        python3 << 'EOF'
import json
import os
import re
from datetime import datetime
from pathlib import Path

def parse_benchmark_results(json_file):
    """Parse Google Benchmark JSON output"""
    with open(json_file, 'r') as f:
        data = json.load(f)

    results = {}
    for bench in data.get('benchmarks', []):
        name = bench.get('name', '')
        if '_mean' in name:  # Only aggregate results
            clean_name = name.replace('_mean', '')
            results[clean_name] = {
                'time_ns': bench.get('cpu_time', 0),
                'iterations': bench.get('iterations', 0),
                'time_unit': bench.get('time_unit', 'ns')
            }
    return results

def calculate_speedup(pool_time, standard_time):
    """Calculate speedup factor"""
    if standard_time > 0:
        return standard_time / pool_time
    return 0

def format_results_table(ubuntu_results, macos_results):
    """Format benchmark results as markdown table"""
    lines = []
    lines.append("## Memory Pool Performance Results")
    lines.append("")
    lines.append(f"> **Last Updated**: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')} UTC")
    lines.append(f"> **Auto-generated by**: GitHub Actions")
    lines.append("")

    # Ubuntu results
    if ubuntu_results:
        lines.append("### Ubuntu 22.04 (x86_64)")
        lines.append("")
        lines.append("| Benchmark | Time (ns) | vs Standard Allocator |")
        lines.append("|-----------|-----------|----------------------|")

        # Key benchmarks to display
        key_benchmarks = [
            ('BM_PoolAllocation_64B', 'BM_StandardAllocation_64B'),
            ('BM_PoolAllocation_256B', 'BM_StandardAllocation_256B'),
            ('BM_PoolAllocation_1KB', 'BM_StandardAllocation_1KB'),
        ]

        for pool_key, std_key in key_benchmarks:
            if pool_key in ubuntu_results and std_key in ubuntu_results:
                pool_time = ubuntu_results[pool_key]['time_ns']
                std_time = ubuntu_results[std_key]['time_ns']
                speedup = calculate_speedup(pool_time, std_time)
                lines.append(f"| {pool_key} | {pool_time:.1f} | {speedup:.1f}x faster |")
                lines.append(f"| {std_key} | {std_time:.1f} | baseline |")

        # Batch operations
        batch_keys = [k for k in ubuntu_results.keys() if 'Batch' in k and 'Pool' in k]
        if batch_keys:
            lines.append("")
            lines.append("#### Batch Operations")
            lines.append("")
            for key in sorted(batch_keys)[:3]:  # Top 3
                time_ns = ubuntu_results[key]['time_ns']
                lines.append(f"| {key} | {time_ns:.1f} | - |")

        lines.append("")

    # macOS results
    if macos_results:
        lines.append("### macOS (ARM64)")
        lines.append("")
        lines.append("| Benchmark | Time (ns) | vs Standard Allocator |")
        lines.append("|-----------|-----------|----------------------|")

        key_benchmarks = [
            ('BM_PoolAllocation_64B', 'BM_StandardAllocation_64B'),
            ('BM_PoolAllocation_256B', 'BM_StandardAllocation_256B'),
            ('BM_PoolAllocation_1KB', 'BM_StandardAllocation_1KB'),
        ]

        for pool_key, std_key in key_benchmarks:
            if pool_key in macos_results and std_key in macos_results:
                pool_time = macos_results[pool_key]['time_ns']
                std_time = macos_results[std_key]['time_ns']
                speedup = calculate_speedup(pool_time, std_time)
                lines.append(f"| {pool_key} | {pool_time:.1f} | {speedup:.1f}x faster |")
                lines.append(f"| {std_key} | {std_time:.1f} | baseline |")

        lines.append("")

    # Key Insights
    lines.append("## Key Performance Insights")
    lines.append("")

    if ubuntu_results:
        pool_64b = ubuntu_results.get('BM_PoolAllocation_64B', {}).get('time_ns', 0)
        std_64b = ubuntu_results.get('BM_StandardAllocation_64B', {}).get('time_ns', 0)
        if pool_64b and std_64b:
            speedup = calculate_speedup(pool_64b, std_64b)
            lines.append(f"- **64B blocks**: Pool is **{speedup:.1f}x faster** than standard allocator (Ubuntu)")

    if macos_results:
        pool_64b = macos_results.get('BM_PoolAllocation_64B', {}).get('time_ns', 0)
        std_64b = macos_results.get('BM_StandardAllocation_64B', {}).get('time_ns', 0)
        if pool_64b and std_64b:
            speedup = calculate_speedup(pool_64b, std_64b)
            lines.append(f"- **64B blocks**: Pool is **{speedup:.1f}x faster** than standard allocator (macOS)")

    lines.append("")
    lines.append("## Recommendations")
    lines.append("")
    lines.append("- **Small blocks (<256B)**: Use memory pool for maximum performance")
    lines.append("- **Large blocks (>1KB)**: Standard allocator comparable, pool still beneficial for reuse")
    lines.append("- **High-frequency allocation**: Memory pool essential for performance")
    lines.append("")

    return "\n".join(lines)

# Main execution
try:
    ubuntu_results = {}
    macos_results = {}

    # Parse Ubuntu results
    ubuntu_file = Path('benchmark-artifacts/memory-pool-results-ubuntu-22.04/memory_pool_results_ubuntu-22.04.json')
    if ubuntu_file.exists():
        ubuntu_results = parse_benchmark_results(ubuntu_file)
        print(f"Parsed {len(ubuntu_results)} Ubuntu benchmarks")

    # Parse macOS results
    macos_file = Path('benchmark-artifacts/memory-pool-results-macos-latest/memory_pool_results_macos-latest.json')
    if macos_file.exists():
        macos_results = parse_benchmark_results(macos_file)
        print(f"Parsed {len(macos_results)} macOS benchmarks")

    # Generate documentation
    if ubuntu_results or macos_results:
        docs_content = format_results_table(ubuntu_results, macos_results)

        # Write to file
        docs_dir = Path('docs')
        docs_dir.mkdir(exist_ok=True)

        output_file = docs_dir / 'MEMORY_POOL_PERFORMANCE.md'
        with open(output_file, 'w') as f:
            f.write(docs_content)

        print(f"Documentation written to {output_file}")
        print("::set-output name=updated::true")
    else:
        print("No benchmark results found to process")
        print("::set-output name=updated::false")

except Exception as e:
    print(f"Error: {e}")
    import traceback
    traceback.print_exc()
    exit(1)
EOF

    - name: Commit and push changes
      if: steps.update_docs.outputs.updated == 'true'
      run: |
        git config --local user.email "github-actions[bot]@users.noreply.github.com"
        git config --local user.name "github-actions[bot]"

        git add docs/MEMORY_POOL_PERFORMANCE.md

        if git diff --staged --quiet; then
          echo "No changes to commit"
        else
          git commit -m "docs: Update memory pool performance results [skip ci]

Auto-generated performance benchmark results from GitHub Actions.

- Platform: Ubuntu 22.04, macOS latest
- Build: Release
- Timestamp: $(date -u '+%Y-%m-%d %H:%M:%S UTC')"

          git push
          echo "âœ… Performance documentation updated and pushed"
        fi

    - name: Create PR comment (for pull requests)
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const path = require('path');

          try {
            const perfFile = path.join('docs', 'MEMORY_POOL_PERFORMANCE.md');
            if (fs.existsSync(perfFile)) {
              const content = fs.readFileSync(perfFile, 'utf8');

              await github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: `## ðŸš€ Memory Pool Performance Results\n\n${content}\n\n---\n*Auto-generated by GitHub Actions*`
              });
            }
          } catch (error) {
            console.log('Could not create PR comment:', error.message);
          }

  summary:
    name: Performance Summary
    needs: [benchmark-memory-pool, update-documentation]
    runs-on: ubuntu-22.04
    if: always()

    steps:
    - name: Generate overall summary
      run: |
        echo "# Memory Pool Performance Benchmark Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "âœ… Benchmarks completed across multiple platforms" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## Artifacts" >> $GITHUB_STEP_SUMMARY
        echo "- Benchmark results uploaded for 90 days retention" >> $GITHUB_STEP_SUMMARY
        echo "- Performance documentation updated in \`docs/MEMORY_POOL_PERFORMANCE.md\`" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## Next Steps" >> $GITHUB_STEP_SUMMARY
        echo "1. Review performance results" >> $GITHUB_STEP_SUMMARY
        echo "2. Compare with previous baselines" >> $GITHUB_STEP_SUMMARY
        echo "3. Investigate any regressions" >> $GITHUB_STEP_SUMMARY
