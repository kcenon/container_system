name: Benchmarks

on:
  push:
    branches: [ main, phase-* ]
    paths:
      - 'benchmarks/**'
      - 'core/**'
      - 'values/**'
      - 'internal/**'
      - '.github/workflows/benchmarks.yml'
  pull_request:
    branches: [ main ]
    paths:
      - 'benchmarks/**'
      - 'core/**'
      - 'values/**'
      - 'internal/**'
  workflow_dispatch:
    inputs:
      save_baseline:
        description: 'Save as baseline'
        required: false
        default: 'false'

jobs:
  benchmark:
    name: Run Performance Benchmarks
    runs-on: ${{ matrix.os }}
    timeout-minutes: 30

    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-22.04, macos-13]
        compiler: [clang]
        build_type: [Release]

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for comparison

    - name: Checkout common_system (optional dependency)
      uses: actions/checkout@v4
      continue-on-error: true
      with:
        repository: kcenon/common_system
        path: common_system
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: Install dependencies (Ubuntu)
      if: runner.os == 'Linux'
      run: |
        sudo apt-get update
        # Install GCC 13 for C++20 <format> support
        sudo apt-get install -y software-properties-common
        sudo add-apt-repository -y ppa:ubuntu-toolchain-r/test
        sudo apt-get update
        sudo apt-get install -y cmake gcc-13 g++-13 ninja-build clang libbenchmark-dev libgtest-dev
        # Set GCC 13 as default for Clang benchmark builds
        sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-13 100
        sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-13 100

    - name: Install dependencies (macOS)
      if: runner.os == 'macOS'
      run: |
        brew install ninja google-benchmark googletest

    - name: Set up compiler (Ubuntu)
      if: runner.os == 'Linux'
      run: |
        echo "CC=clang" >> $GITHUB_ENV
        echo "CXX=clang++" >> $GITHUB_ENV
        # Force Clang to use GCC 13 headers for std::format support
        echo "CXXFLAGS=-isystem /usr/include/c++/13 -isystem /usr/include/x86_64-linux-gnu/c++/13 --gcc-toolchain=/usr" >> $GITHUB_ENV
        echo "CFLAGS=--gcc-toolchain=/usr" >> $GITHUB_ENV

    - name: Set up compiler (macOS)
      if: runner.os == 'macOS'
      run: |
        echo "CC=clang" >> $GITHUB_ENV
        echo "CXX=clang++" >> $GITHUB_ENV

    - name: Configure CMake
      run: |
        # Build CMake argument array
        CMAKE_FLAGS=(
          -DCMAKE_BUILD_TYPE=${{ matrix.build_type }}
          -DCONTAINER_BUILD_BENCHMARKS=ON
          -DBUILD_TESTS=OFF
          -DBUILD_CONTAINER_SAMPLES=OFF
        )

        # Add compiler flags from environment if set
        if [ -n "$CXXFLAGS" ]; then
          CMAKE_FLAGS+=(-DCMAKE_CXX_FLAGS="$CXXFLAGS")
        fi
        if [ -n "$CFLAGS" ]; then
          CMAKE_FLAGS+=(-DCMAKE_C_FLAGS="$CFLAGS")
        fi

        cmake -B build -S . -GNinja "${CMAKE_FLAGS[@]}"

    - name: Check if benchmarks are enabled
      id: check_benchmarks
      run: |
        if grep -q "# Benchmarks temporarily disabled" CMakeLists.txt; then
          echo "benchmarks_enabled=false" >> $GITHUB_OUTPUT
          echo "::warning::Benchmarks are temporarily disabled - skipping benchmark execution"
        else
          echo "benchmarks_enabled=true" >> $GITHUB_OUTPUT
        fi

    - name: Build benchmarks
      if: steps.check_benchmarks.outputs.benchmarks_enabled == 'true'
      run: cmake --build build --config ${{ matrix.build_type }} --target container_benchmarks -j

    - name: Run benchmarks
      if: steps.check_benchmarks.outputs.benchmarks_enabled == 'true'
      run: |
        cd build/bin
        ./container_benchmarks \
          --benchmark_format=json \
          --benchmark_out=benchmark_results_${{ matrix.os }}.json \
          --benchmark_repetitions=3 \
          --benchmark_report_aggregates_only=true

    - name: Run benchmarks (console output)
      if: steps.check_benchmarks.outputs.benchmarks_enabled == 'true'
      run: |
        cd build/bin
        ./container_benchmarks \
          --benchmark_filter="Container_Create|Container_AddValue|Serialize_Small|Value_Create" \
          --benchmark_repetitions=3

    - name: Upload benchmark results
      if: steps.check_benchmarks.outputs.benchmarks_enabled == 'true'
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-${{ matrix.os }}
        path: build/bin/benchmark_results_${{ matrix.os }}.json
        retention-days: 30

    - name: Save baseline (if requested)
      if: github.event.inputs.save_baseline == 'true' && github.ref == 'refs/heads/main'
      run: |
        mkdir -p benchmarks/baselines
        cp build/bin/benchmark_results_${{ matrix.os }}.json \
           benchmarks/baselines/baseline_${{ matrix.os }}_$(date +%Y%m%d).json

    - name: Compare with baseline (if exists)
      if: hashFiles('benchmarks/baselines/baseline_${{ matrix.os }}_*.json') != ''
      run: |
        echo "Comparing with baseline..."
        # Install compare.py from Google Benchmark tools
        pip3 install --user scipy

        # Find latest baseline
        BASELINE=$(ls -t benchmarks/baselines/baseline_${{ matrix.os }}_*.json | head -1)

        if [ -f "$BASELINE" ]; then
          echo "Baseline: $BASELINE"
          echo "Current: build/bin/benchmark_results_${{ matrix.os }}.json"

          # Download compare.py
          curl -O https://raw.githubusercontent.com/google/benchmark/main/tools/compare.py
          chmod +x compare.py

          # Run comparison
          python3 compare.py \
            "$BASELINE" \
            build/bin/benchmark_results_${{ matrix.os }}.json
        fi

    - name: Check for performance regression
      run: |
        echo "Performance regression detection: TBD"
        echo "Will be implemented in Phase 1 with baseline establishment"

  report:
    name: Generate Benchmark Report
    needs: benchmark
    runs-on: ubuntu-22.04
    if: always()

    steps:
    - name: Download all artifacts
      uses: actions/download-artifact@v4

    - name: Generate summary
      run: |
        echo "# Benchmark Results Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## Phase 0: Baseline Measurement" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        for dir in benchmark-results-*; do
          if [ -d "$dir" ]; then
            echo "### $dir" >> $GITHUB_STEP_SUMMARY
            if [ -f "$dir/benchmark_results_"*.json ]; then
              echo "✅ Benchmarks completed" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
            else
              echo "❌ No results found" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
            fi
          fi
        done

        echo "## Next Steps" >> $GITHUB_STEP_SUMMARY
        echo "- Review benchmark results" >> $GITHUB_STEP_SUMMARY
        echo "- Document baseline in docs/BASELINE.md" >> $GITHUB_STEP_SUMMARY
        echo "- Set performance targets for Phase 1" >> $GITHUB_STEP_SUMMARY
